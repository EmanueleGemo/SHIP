{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4adfa0",
   "metadata": {},
   "source": [
    "# SHIP: creating a model for a component\n",
    "\n",
    "This section focuses on the definition of a component's model.\n",
    "\n",
    "### Data convention: recall\n",
    "We remind the convention by which data should be structured.\n",
    "\n",
    "Input data should be a tensor of size (batch_size,nU), with nU being the number of units. For neurongroups in particular, the number of units is the number of components of the group. The output is expected to have the same format, as is the various group states (though there is more flexibility on the data processed internally).\n",
    "\n",
    "For sake of simplicity, synapsegroups also perform what is part of the dendritic integration. In fact, if a synapsegroup connects a neurongroup A, having number of units NA = 3, and a neurongoup B of NB = 5, a synapsegroup should have a number of units nU = 3x5 = 15 components, but expects an input of shape (batch_size, NA) and delivers an output of shape (batch_size, NB). \n",
    "These are convention in place so to lower the size of data that must be shuttled between groups, but newer versions of SHIP might put in place a more flexible framework, separating synaptic functionality and dendritic integration. This would also simplify the creation of models intended for different funcitonalities other than the synaptic and neuronal ones.\n",
    "\n",
    "### Preliminary action - adding folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e02bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\') # to be edited by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fd517",
   "metadata": {},
   "source": [
    "\n",
    "## Generic model\n",
    "\n",
    "### methods and properties of a model's class\n",
    "To model each group, SHIP uses a **group** class, containing a scaffold of method intended to be edited easily.  We anticipate that, in order to facilitate certain internal operations, the **group** class is further divided in two distinct classes, the **neurongroup**, and **synapsegroup**, each applicable for either neurons or synapses.\n",
    "\n",
    "These scaffold of methods that each group needs editing for are the following:\n",
    "- `advance_timestep`, which is the heart of the model. This method is expected to deliver a time-step-dependent output each time it is called, generated according to a user-defined set of equations. This method can optionally intake a (time-step-dependent) input, and contain any number of internal states.\n",
    "- `set_initial_state`, that is intended to perform any (non-automated) initialization operation to set up the model to be ready for the temporal evolution emulation via the `advance_timestep` method (a sort of zeroth-step, if you find it easier to see it like that!)\n",
    "- `time_dep`, a method that precalculates the part of the model equations that explicitly depend on the time-step. This method has been put in place to enable one to reduce the reiterated calculations during the `advance_timestep` method.\n",
    "\n",
    "Any additional method can indeed be included in the class definition, and called within the above methods.\n",
    "\n",
    "\n",
    "\n",
    "It is also useful to explicitly map the variables and parameters. A dict property for the variables, `group.variables`, and one for the parameters, `group.parameters`, take this role. The dict can contain merely the keys (that match the desired variables and parameters, as typed within the equations). However, it is also reccommended to use, as dict values, the default values that variables/parameters can take. See an example below, for the 1st order leaky synapse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ad8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import rand\n",
    "from SHIP.group import synapsegroup # useful as it pre-set the group to find the number of components as a function of source and target\n",
    "\n",
    "class lS_1o(synapsegroup):\n",
    "    variables = {'_I__': 0} # synapse current <--> internal state\n",
    "    parameters = {'tau_alpha__': 8e-3, # temporal constant [s]\n",
    "                  'w__': rand, # synaptic weight\n",
    "                  'w_scale': 1} # global scaling factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769290",
   "metadata": {},
   "source": [
    "Note that the syntetic notation using underscores and generator functions can be employed here as well (see Tutorial_1 for more info).\n",
    "\n",
    "### Automated initialization: information\n",
    "We referred to non-automated initialization operations, and here recall some useful info about this.\n",
    "\n",
    "During the network building stage, each group can accept user-defined values for a list of arguments matching the model's *variables* or *parameters*. \n",
    "- *variables* are assumed to be flexible containers, changing during the temporal evolution; they may match with the models' states. Those are set in the model when calling the `network.shallow_init` method, which is automatically done when calling `network.run`, but it can be avoided direclty calling `network.run_` or `network.run_no_out` (see Tutorial_2 for more information).\n",
    "- *parameters* should remain static during the temporal evolution, and are always reset to the user-defined values when the user explicitly calls `network.init()` (but not when calling `network.run`).\n",
    "\n",
    "Summarizing, `network.init` sets the user-provided values for the parameters of each group; `network.shallow_init` does the same, but for the user-provided variables. It is therefore not necessary to repeat these on the `set_initial_state` method for the group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11b448",
   "metadata": {},
   "source": [
    "## Example: 1st order leaky synapse\n",
    "\n",
    "We reprise the 1st order leaky synapse to give an example as to how to code a component model.\n",
    "\n",
    "As shown before, it is quite useful to explicitly report parameters and values within the reserved dicts, as these are automatically used to set the values provided during the network building stage.\n",
    "\n",
    "At this point, we briefly digress to mention what to expect from this model.\n",
    "A 1st order leaky synapse sees the variation of the current in time as follows:\n",
    "\n",
    "    dI/dt = I / tau_alpha\n",
    "\n",
    "Solving the differential equation, we obtain this solution:\n",
    "\n",
    "    I(t) = I x exp(-t/tau_alpha)\n",
    "\n",
    "Note that this solution can be easily transformed in a time-discrete equation, readily applicable to our model, by replacing the arbitrary time t with a time-step duration delta_t:\n",
    "\n",
    "    I(t+delta_t) = I(t) x exp(-delta_t/tau_alpha)\n",
    "    \n",
    "We also need to take care of the synaptic input.\n",
    "We can simply assume that, at the arrival of a pre-synaptic spike S, the current raises by the value w. This is an instantaneous event.\n",
    "\n",
    "    I(t+delta_t) = I(t) x exp(-delta_t/tau_alpha) + wS\n",
    "\n",
    "At this stage, we can further improve this model, by calculating the delta_t-dependent output, and we can easily do so by integrating this equation along the time-step duration (easily for this equation; more complex models might need some work). This lead to the following solution:\n",
    "\n",
    "    I_integral(t) = I(t) x [tau_alpha x (1-exp(-delta_t/tau_alpha))]\n",
    "    \n",
    "Therefore, we can already define a stub of the `advance_timestep` method, that carries out these operations. Note that all variables and parameters are to be stored as the group properties, and thus these must be addressed as `self.name_of_the_property` for them to be properly set and remembered from time-step to time-step.\n",
    "\n",
    "Note also that, in the current version of SHIP, the time-step size `dt` is already available as a property of each group (as are the `batch_size` and number of time-steps `nts`).\n",
    "\n",
    "\n",
    "Let us write a first stub of the `advance_timestep` function.\n",
    "We need the following properties:\n",
    "- current I (a variable)\n",
    "- current integral I_int \n",
    "- synaptic weight w (a parameter)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b646f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import (tensor,exp)\n",
    "def advance_timestep(self,S = 0): # draft version\n",
    "    # I: point-current (time-step dependent)\n",
    "    # I_int: integral of the point current for each time-step\n",
    "    \n",
    "    self.I = self.I*tensor(-self.dt/self.tau_alpha).exp()  # <-- time-step current decrease\n",
    "    \n",
    "    self.I = self.I + S*self.w # <-- adding perturbation from S\n",
    "    \n",
    "    self.I_int = self.I* self.tau_alpha*(1-tensor(-self.dt/self.tau_alpha).exp()) # <-- calc output\n",
    "    \n",
    "    return self.I_int    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb85bf7",
   "metadata": {},
   "source": [
    "Already at this stage, we see how we can use `time_dep` to avoid repeating a few parts of the calculation above. We can use this to define two parameters, `alphaA` and `alphaB`, that take care of the time-step size dependency, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70a15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_dep(self): # here we pre-calculate part of the solution of the draft\n",
    "    self.alphaA = tensor(-self.dt/self.tau_alpha).exp()\n",
    "    self.alphaB = self.tau_alpha*(1-self.alphaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d142e6",
   "metadata": {},
   "source": [
    "and we can modify `advance_timestep` so to use the new properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3732281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advance_timestep(self,S = 0): # second draft\n",
    "    # I: point-current (time-step dependent)\n",
    "    \n",
    "    # here we change the state I\n",
    "    self.I = self.I* self.alphaA + S*self.w # both time decrease and perturbation effect \n",
    "    \n",
    "    # and, since the output is not necessarily a group state, we do not store it internally\n",
    "    # instead we just return the current integral value\n",
    "    return self.I*self.alphaB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56edf5dc",
   "metadata": {},
   "source": [
    "A new class including ``time_dep`` and `advance_timestep` as shown would already be functional.\n",
    "However, it would not necessarily work for groups having a number of components greater than one, as the equations are not explicitly tailored to work with tensors of arbitrary size.\n",
    "\n",
    "Below, we adjust ``advance_timestep`` in order to have a method that can work with 2D matrices of components, mapping a source and a target of arbitrary sizes.\n",
    "\n",
    "We also include the post-synaptic integration of the currents targeting each individual post-synaptic neuron. This is not a feature of the synaptic model, but it is useful to put it here so to decrease the model's output size (though - this is an arbitrary choice!)\n",
    "\n",
    "We note that each group have a few properties dedicated to store the size of their component pool, the one of the sources, and the one of the targets. Those are, respectively, `self.N` (own number of components), `self.Ns` (source number of components), and `self.Nt` (target number of components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c80a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advance_timestep(self,S = 0): # operative version\n",
    "    # I: point-current (time-step dependent)\n",
    "    \n",
    "    # the input S is expected to be a tensor of size(batch_size,self.Ns); we need to expand it to\n",
    "    # apply the spikes to each \"column\" of synapses targeting a different post-synaptic neuron\n",
    "    # since we modify the method - we also propose a w_scale property, that act as a global scaling \n",
    "    # factor for the synaptic weights (default = 1, so to not do anything)\n",
    "    \n",
    "    local_input = S.unsqueeze(-1).expand(self.batch_size,self.Ns,self.Nt)*self.w*self.w_scale\n",
    "    # see infos on the functions unsqueeze and expand in https://pytorch.org/docs/stable/torch.html\n",
    "    \n",
    "    # here we change the state I, that is assumed to be a tensor of size (self.Ns, self.Nt)\n",
    "    self.I = self.I* self.alphaA +  local_input \n",
    "\n",
    "    # now we also edit the output, sending out the sum of the currents along each column of synapses\n",
    "    # targeting each individual post-synaptic neuron\n",
    "    return (self.I*self.alphaB).sum(dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362db86",
   "metadata": {},
   "source": [
    "The latter is the method that can be employed as-is within a 1st-order leaky synapse group. There is no strict need to use the `set_initial_state` method (though we use it to check the datatype of certain parameters, but that's not as important as the rest of the calculations shown here).\n",
    "\n",
    "To summarize:\n",
    "- we have a model's state, the post-synaptic current `I`, that is a tensor of size (`Ns`,`Nt`); the state will be remembered time-step after time-step, as it is stored as a group property\n",
    "- `time_dep` calculates a couple of parameters explicitly dependent on the time-step `dt`, namely `alphaA` and `alphaB`, also permanently available as group properties\n",
    "- the state `I` varies according to the temporal dependency (multiplication of `I` by `alphaA`)\n",
    "- the state is also updated according to the (reshaped) set of inputs `S`\n",
    "- the output is the current integral along the time-step  (multiplication of `I` by `alphaB`), summed along the columns targeting each individual post-synaptic neuron\n",
    "\n",
    "We note that our implementation calculates the variation of the current, then applies a spike perturbation, and only then calculates the integral (thus including the spike effect). These are arbitrary choices, that may or may not fit the intended scope of the end-user, which is tasked to check and possibly amend the group class where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3ec08",
   "metadata": {},
   "source": [
    "## Example: leaky-integrate and fire neuron\n",
    "\n",
    "The LIF neuron is a well-known model that fits extremely well in the SHIP framework, due to the simplicity of its equations. Let's rapidly have a look at the equations.\n",
    "\n",
    "The LIF neuron possesses an internal state, the membrane potential U, that varies through time as follows:\n",
    "\n",
    "    dU/dt = U/tau_beta\n",
    "    \n",
    "The similarity with the 1st order leaky synapse is evident. Solving this 1st order ordinary differential equation, we get to the following solution:\n",
    "\n",
    "    U(t+dt) = U(t)*exp(-dt/tau_beta)\n",
    "    \n",
    "Again, similarly to what happens with the leaky synapse model, we now include the effect of the synaptic current on the membrane potential:\n",
    "\n",
    "    U(t+dt) = U(t)*exp(-dt/tau_beta) + S\n",
    "    \n",
    "(note that S is here expected to be the current integral along the temporal duration t -> t+dt)\n",
    "    \n",
    "We now need to indicate what happens when placing a potential threhsold ``thr``, after which the neuron fires and resets its potential. It is fairly easy using pseudocode:\n",
    "\n",
    "    if U(t) > thr:\n",
    "        U(t) = 0\n",
    "        output = 1\n",
    "    else:\n",
    "        output = 0\n",
    "    \n",
    "We can define an initial model stub that carries out the operations above.\n",
    "We already start defining the temporal constant `beta` as the exponential of ``(-dt/tau_beta)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a7e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SHIP.group import neurongroup # so for the platform to know how to handle the internal com\n",
    "\n",
    "class lifN(neurongroup):  # first draft\n",
    "    \n",
    "    variables = {'_u_': 0} # membrane potential <-- internal state\n",
    "    parameters = {'thr': 1, # threshold potential - the neuron fires once u overcomes its value\n",
    "                  'tau_beta_': 1e-3} # temporal constant [s]\n",
    "    \n",
    "    def time_dep(self):\n",
    "        self.beta = tensor(-self.dt/self.tau_beta).exp()\n",
    "\n",
    "    def advance_timestep(self,S=0):\n",
    "        self.u = self.u*self.beta+S #<-- state variation\n",
    "        spikes = (self.u-self.thr)<0 #<-- output determination\n",
    "        self.u[spikes.detach()] = 0 #<-- state variation (reset after spike); \n",
    "        # detach() is here used to take care of possible issues during the training process (see torch manual for details)\n",
    "        return spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b784c",
   "metadata": {},
   "source": [
    "This draft is already functional, as it already contains all the LIF model operations, structured within the available methods. \n",
    "\n",
    "However, to be fully integrated with SHIP (especially to deal with the training part), we need to modify it.\n",
    "\n",
    "### Activator\n",
    "In particular, SHIP needs to know where to locate the neuron activation function, so to apply its modified surrogate-gradient to allow back-propagation.\n",
    "Therefore, it is advisable to use the reserved method, `activator`, to carry out this part of the model. See below how this is used for the lif model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758e6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lifN(neurongroup):  # second draft    \n",
    "    variables = {'_u_': 0}\n",
    "    parameters = {'thr': 1,\n",
    "                  'tau_beta_': 1e-3}\n",
    "    \n",
    "    def activator(self, arg):\n",
    "        return arg<0\n",
    "    \n",
    "    def time_dep(self):\n",
    "        self.beta = tensor(-self.dt/self.tau_beta).exp()\n",
    "\n",
    "    def advance_timestep(self,S=0):\n",
    "        self.u = self.u*self.beta+S #<-- state variation\n",
    "        spikes = self.activator(self.u-self.thr) #<-- output determination\n",
    "        self.u[spikes.detach()] = 0 #<-- state variation (reset after spike); \n",
    "        # detach() is here used to take care of possible issues during the training process (see torch manual for details)\n",
    "        return spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff788b",
   "metadata": {},
   "source": [
    "Now, with this amendment, the neuron group can support all the PyTorch training functionalities enabled by SHIP. Of course, the opposite is also true (no activator -> no Torch training), and this formalism is not necessary if the user does not plan to train the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15828903",
   "metadata": {},
   "source": [
    "###  integrate\n",
    "\n",
    "Another element that needs modification is the presence of the `integrate` property, which is a boolean tensor of same size as `u`, and enables the model to delegate the management of the \"does the neuron integrate?\" functionality (a.k.a. refractoriness).\n",
    "\n",
    "SHIP allows to add the refractoriness functionality on neuron models, but it does so by applying necessary modifications to the `integrate` group's property. Therefore, to fully support this feature, the neuron models need to make proper use of this. \n",
    "\n",
    "Below, we show how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c712757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advance_timestep(self,S=0):\n",
    "    self.u = self.integrate*self.u*self.beta+S \n",
    "    # state variation, also including the integrate selector\n",
    "    # now we do not need to manually-reset u after spike!\n",
    "    spikes = self.activator(self.u-self.thr) #<-- output determination\n",
    "    self.integrate = ~spikes.detach() # but we need to determine when to integrate, like this!\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858961c8",
   "metadata": {},
   "source": [
    "Now, if a ``refractory(lifN)`` group is added to a **network**, it includes the refractory functionality, provided that the user sets the argument *refr_time*, i.e. the refractory time. To do so, SHIP will set the variable `self.integrate` to zero during each time-step that the corresponding neuron need not to integrate any input.\n",
    "\n",
    "This is an arbitrary approach, and possibly not the most straightforward - but delegating the integrating role to just one variable (``integrate``) allows us to leave all the remaining bits of the model as they are, including having the untouched local input `S` in `advance_timestep` (in case other operations needs to be carried out)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
